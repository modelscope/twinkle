# 服务端（Server）

## 启动方式

Server 统一通过 `launch_server` 函数或 CLI 命令启动，配合 YAML 配置文件。

### 方式一：Python 脚本启动

```python
# server.py
import os
from twinkle.server import launch_server

# 获取配置文件路径（与脚本同目录的 server_config.yaml）
file_dir = os.path.abspath(os.path.dirname(__file__))
config_path = os.path.join(file_dir, 'server_config.yaml')

# 启动服务，此调用将阻塞直到服务关闭
launch_server(config_path=config_path)
```

### 方式二：命令行启动

```bash
# 启动 Twinkle 原生 Server
python -m twinkle.server --config server_config.yaml

# 启动 Tinker 兼容 Server
python -m twinkle.server --config server_config.yaml --server-type tinker
```

CLI 支持的参数：

| 参数 | 说明 | 默认值 |
|------|------|-------|
| `-c, --config` | YAML 配置文件路径（必须） | — |
| `-t, --server-type` | Server 模式：`twinkle` 或 `tinker` | `twinkle` |
| `--namespace` | Ray 命名空间 | tinker 模式默认 `twinkle_cluster` |
| `--no-wait` | 不阻塞等待（守护模式） | `False` |
| `--log-level` | 日志级别 | `INFO` |

## YAML 配置详解

配置文件定义了 Server 的完整部署方案，包括 HTTP 监听、应用组件和资源分配。

### Twinkle Server + Transformers 后端

```yaml
# server_config.yaml — Twinkle 原生协议 + Transformers 后端

# 协议类型：twinkle 原生协议
server_type: twinkle

# HTTP 代理位置：EveryNode 表示每个 Ray 节点运行一个代理（多节点场景推荐）
proxy_location: EveryNode

# HTTP 监听配置
http_options:
  host: 0.0.0.0        # 监听所有网络接口
  port: 8000            # 服务端口号

# 应用列表：每个条目定义一个部署在 Server 上的服务组件
applications:

  # 1. TwinkleServer：中央管理服务
  # 负责处理客户端连接、训练运行跟踪、检查点管理等
  - name: server
    route_prefix: /server          # API 路径前缀
    import_path: server            # 内置组件标识
    args:                          # 无额外参数
    deployments:
      - name: TwinkleServer
        autoscaling_config:
          min_replicas: 1                # 最小副本数
          max_replicas: 1                # 最大副本数
          target_ongoing_requests: 128   # 每副本目标并发请求数
        ray_actor_options:
          num_cpus: 0.1                  # 此 Actor 分配的 CPU 资源

  # 2. Model 服务：承载基座模型
  # 执行前向传播、反向传播等训练计算
  - name: models-Qwen2.5-7B-Instruct
    route_prefix: /models/Qwen/Qwen2.5-7B-Instruct   # 模型的 REST 路径
    import_path: model
    args:
      use_megatron: false                              # 使用 Transformers 后端
      model_id: "ms://Qwen/Qwen2.5-7B-Instruct"      # ModelScope 模型标识
      adapter_config:                                  # LoRA 适配器配置
        per_token_adapter_limit: 30   # 同时可激活的最大 LoRA 数量
        adapter_timeout: 1800         # 空闲适配器超时卸载时间（秒）
      nproc_per_node: 2               # 每节点 GPU 进程数
      device_group:                   # 逻辑设备组
        name: model
        ranks: [0, 1]                 # 使用的 GPU 卡号
        device_type: cuda
      device_mesh:                    # 分布式训练网格
        device_type: cuda
        mesh: [0, 1]                  # 网格中的设备索引
        mesh_dim_names: ['dp']        # 网格维度：dp=数据并行
    deployments:
      - name: ModelManagement
        autoscaling_config:
          min_replicas: 1
          max_replicas: 1
          target_ongoing_requests: 16
        ray_actor_options:
          num_cpus: 0.1

  # 3. Processor 服务：数据预处理
  # 在 CPU 上执行 tokenization、模板转换等预处理任务
  - name: processor
    route_prefix: /processors
    import_path: processor
    args:
      nproc_per_node: 2               # 每节点处理器 worker 数
      ncpu_proc_per_node: 2           # 每节点 CPU 进程数
      device_group:
        name: model
        ranks: 2
        device_type: CPU
      device_mesh:
        device_type: CPU
        mesh: [0, 1]
        mesh_dim_names: ['dp']
    deployments:
      - name: ProcessorManagement
        autoscaling_config:
          min_replicas: 1
          max_replicas: 1
          target_ongoing_requests: 128
        ray_actor_options:
          num_cpus: 0.1
```

### Twinkle Server + Megatron 后端

与 Transformers 后端的区别仅在 Model 服务的 `use_megatron` 参数：

```yaml
  # Model 服务 — Megatron 后端
  - name: models-Qwen2.5-7B-Instruct
    route_prefix: /models/Qwen/Qwen2.5-7B-Instruct
    import_path: model
    args:
      use_megatron: true                               # 使用 Megatron-LM 后端
      model_id: "ms://Qwen/Qwen2.5-7B-Instruct"
      nproc_per_node: 2
      device_group:
        name: model
        ranks: [0, 1]
        device_type: cuda
      device_mesh:
        device_type: cuda
        mesh: [0, 1]
        mesh_dim_names: ['dp']
```

> **注意**：Megatron 后端不需要 `adapter_config`（LoRA 适配器管理由 Megatron 内部处理）。

### Tinker 兼容 Server 配置

Tinker 兼容模式的主要区别：
- `server_type` 设为 `tinker`
- `route_prefix` 使用 `/api/v1` 前缀（Tinker 协议规范）
- 可额外配置 Sampler 服务（用于推理采样）

```yaml
# server_config.yaml — Tinker 兼容协议

server_type: tinker

proxy_location: EveryNode

http_options:
  host: 0.0.0.0
  port: 8000

applications:

  # 1. TinkerCompatServer：中央 API 服务
  - name: server
    route_prefix: /api/v1              # Tinker 协议 API 前缀
    import_path: server
    args:
    deployments:
      - name: TinkerCompatServer
        autoscaling_config:
          min_replicas: 1
          max_replicas: 1
          target_ongoing_requests: 128
        ray_actor_options:
          num_cpus: 0.1

  # 2. Model 服务（Megatron 后端示例）
  - name: models-Qwen2.5-0.5B-Instruct
    route_prefix: /api/v1/model/Qwen/Qwen2.5-0.5B-Instruct
    import_path: model
    args:
      use_megatron: true
      model_id: "ms://Qwen/Qwen2.5-0.5B-Instruct"
      nproc_per_node: 2
      device_group:
        name: model
        ranks: [0, 1]
        device_type: cuda
      device_mesh:
        device_type: cuda
        mesh: [0, 1]
        mesh_dim_names: ['dp']
    deployments:
      - name: ModelManagement
        autoscaling_config:
          min_replicas: 1
          max_replicas: 1
          target_ongoing_requests: 16
        ray_actor_options:
          num_cpus: 0.1

  # 3. Sampler 服务（可选，用于推理采样）
  - name: sampler-Qwen2.5-0.5B-Instruct
    route_prefix: /api/v1/sampler/Qwen/Qwen2.5-0.5B-Instruct
    import_path: sampler
    args:
      model_id: "ms://Qwen/Qwen2.5-0.5B-Instruct"
      nproc_per_node: 1
      sampler_type: vllm              # 推理引擎：vllm（高性能）或 torch
      engine_args:                    # vLLM 引擎参数
        max_model_len: 4096           # 最大序列长度
        gpu_memory_utilization: 0.5   # GPU 显存使用比例
        enable_lora: true             # 支持推理时加载 LoRA
      device_group:
        name: sampler
        ranks: [0]
        device_type: cuda
      device_mesh:
        device_type: cuda
        mesh: [0]
        mesh_dim_names: ['dp']
    deployments:
      - name: SamplerManagement
        autoscaling_config:
          min_replicas: 1
          max_replicas: 1
          target_ongoing_requests: 16
        ray_actor_options:
          num_cpus: 0.1
          num_gpus: 1                 # Sampler 需要独立 GPU
```

## 配置项说明

### 应用组件（import_path）

| import_path | Twinkle 模式 | Tinker 模式 | 说明 |
|-------------|-------------|------------|------|
| `server` | ✅ | ✅ | 中央管理服务，处理训练运行和检查点 |
| `model` | ✅ | ✅ | 模型服务，承载基座模型进行训练 |
| `processor` | ✅ | ❌ | 数据预处理服务（仅 Twinkle 模式，Tinker 模式需在本地处理） |
| `sampler` | ✅ | ✅ | 推理采样服务 |

### device_group 与 device_mesh

- **device_group**：定义逻辑设备组，指定使用哪些 GPU 卡
- **device_mesh**：定义分布式训练网格，控制并行策略

```yaml
device_group:
  name: model          # 设备组名称
  ranks: [0, 1]        # GPU 卡号列表
  device_type: cuda     # 设备类型：cuda / CPU

device_mesh:
  device_type: cuda
  mesh: [0, 1]                    # 网格中的设备索引
  mesh_dim_names: ['dp']          # 维度名称，常用：dp(数据并行), tp(张量并行), pp(流水线并行)
```
