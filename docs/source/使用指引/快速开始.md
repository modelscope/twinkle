<div align="center">

## âœ¨ Twinkleæ˜¯ä»€ä¹ˆï¼Ÿ

å¤§æ¨¡å‹è®­ç»ƒç»„ä»¶åº“ã€‚åŸºäº PyTorchï¼Œæ›´ç®€æ´ã€æ›´çµæ´»ã€ç”Ÿäº§å°±ç»ªã€‚

<p align="center">
ğŸ§© <b>æ¾è€¦åˆæ¶æ„</b> Â· æ ‡å‡†åŒ–æ¥å£<br>
ğŸš€ <b>å¤šè¿è¡Œæ¨¡å¼</b> Â· torchrun / Ray / HTTP<br>
ğŸ”Œ <b>å¤šæ¡†æ¶å…¼å®¹</b> Â· Transformers / Megatron<br>
ğŸ‘¥ <b>å¤šç§Ÿæˆ·æ”¯æŒ</b> Â· å•åŸºåº§æ¨¡å‹éƒ¨ç½²
</p>

</div>

## twinkleé€‚é…æ€§

twinkleå’Œ[ms-swift](https://github.com/modelscope/ms-swift)éƒ½æ˜¯æ¨¡å‹è®­ç»ƒæ¡†æ¶ï¼Œä½†äºŒè€…çš„ç‰¹æ€§æœ‰å¾ˆå¤§ä¸åŒï¼Œå¼€å‘è€…å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚é€‰æ‹©ã€‚

### ä½•æ—¶é€‰æ‹©twinkle

- å¦‚æœä½ æ˜¯å¤§æ¨¡å‹çš„åˆå­¦è€…ï¼Œå¸Œæœ›æ›´å¥½åœ°äº†è§£æ¨¡å‹æœºåˆ¶å’Œæ¨¡å‹è®­ç»ƒæ–¹æ³•
- å¦‚æœä½ æ˜¯å¤§æ¨¡å‹ç ”ç©¶è€…ï¼Œå¸Œæœ›å®šåˆ¶æ¨¡å‹æˆ–è€…è®­ç»ƒæ–¹æ³•
- å¦‚æœä½ å–„äºç¼–å†™training loopï¼Œå¸Œæœ›å®šåˆ¶è®­ç»ƒè¿‡ç¨‹
- å¦‚æœä½ æ˜¯Bç«¯ï¼Œå¸Œæœ›æä¾›å•†ä¸šåŒ–è®­ç»ƒå¹³å°
- å¦‚æœä½ ç¼ºå°‘è®­ç»ƒç¡¬ä»¶ï¼Œå¸Œæœ›ä½¿ç”¨ç¤¾åŒºèµ„æº

### ä½•æ—¶é€‰æ‹©ms-swift

- å¦‚æœä½ ä¸å…³å¿ƒè®­ç»ƒè¿‡ç¨‹ï¼Œå¸Œæœ›ä»…æä¾›æ•°æ®é›†ä¾¿å¯å®Œæˆè®­ç»ƒ
- å¦‚æœä½ éœ€è¦æ›´å¤šçš„æ¨¡å‹æ”¯æŒå’Œæ•°æ®é›†ç§ç±»
- å¦‚æœä½ éœ€è¦Embeddingã€Rerankerã€Classificationç­‰å¤šç§ç±»å‹çš„è®­ç»ƒ
- å¦‚æœä½ éœ€è¦æ¨ç†ã€éƒ¨ç½²ã€é‡åŒ–ç­‰å…¶ä»–èƒ½åŠ›
- å¦‚æœä½ å¯¹æ–°æ¨¡å‹çš„è®­ç»ƒæ”¯æŒæ•æ„Ÿï¼Œswiftä¼šä¿è¯day-0çš„æ›´æ–°èƒ½åŠ›

## twinkleçš„å¯å®šåˆ¶ç»„ä»¶

åœ¨twinkleçš„è®¾è®¡ä¸­ï¼Œtorchrunã€rayã€httpçš„è®­ç»ƒä½¿ç”¨åŒæ ·çš„APIï¼Œå¹¶åˆ†äº«ç›¸åŒçš„ç»„ä»¶å’Œè¾“å…¥è¾“å‡ºç»“æ„ã€‚å› æ­¤å…¶å¾ˆå¤šç»„ä»¶å¯ä»¥ç”±å¼€å‘è€…è‡ªå®šä¹‰æ¥å®ç°æ–°çš„ç®—æ³•å¼€å‘ã€‚

ä¸‹é¢æˆ‘ä»¬åˆ—å‡ºæ¨èå®šåˆ¶çš„ç»„ä»¶åˆ—è¡¨ï¼š

| ç»„ä»¶åç§°                  | åŸºç±»                                         | è¯´æ˜                                 |
|-----------------------|--------------------------------------------|------------------------------------|
| æŸå¤±                    | twinkle.loss.Loss                          | ç”¨äºå®šä¹‰æ¨¡å‹è®­ç»ƒåçš„æŸå¤±å‡½æ•°                     |
| æŒ‡æ ‡                    | twinkle.metric.Metric                      | ç”¨äºå®šä¹‰æ¨¡å‹è®­ç»ƒçš„è¯„ä»·ä½“ç³»                      |
| Optimizer/LRScheduler | åŸºäºPyTorch                                  | ç”¨äºå®šä¹‰æ¨¡å‹è®­ç»ƒçš„ä¼˜åŒ–å™¨å’ŒLRè¡°å‡å™¨                 | 
| è¡¥ä¸                    | twinkle.patch.Patch                        | ç”¨äºä¿®å¤æ¨¡å‹è®­ç»ƒè¿‡ç¨‹çš„è¡¥ä¸                      |
| é¢„å¤„ç†å™¨                  | twinkle.preprocessor.Preprocessor          | ç”¨äºå¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†(ETL)ï¼Œå¹¶è¿”å›templateå¯ç”¨çš„æ ‡å‡†æ ¼å¼ |
| è¿‡æ»¤å™¨                   | twinkle.preprocessor.Filter                | ç”¨äºå¯¹åŸå§‹æ•°æ®è¿›è¡Œåˆç†æ€§è¿‡æ»¤                     |
| ä»»åŠ¡æ•°æ®å¤„ç†å™¨               | twinkle.processor.InputProcessor           | ç”¨äºå¯¹æ¨¡å‹è¾“å…¥è½¬ä¸ºå„ä»»åŠ¡éœ€è¦çš„æ•°æ®ï¼Œå¹¶æ·»åŠ é¢å¤–å­—æ®µ          |
| æ¨¡å‹                    | twinkle.model.TwinkleModel                 | å¤§æ¨¡å‹æœ¬èº«                              |
| é‡‡æ ·å™¨                   | twinkle.sampler.Sampler                    | é‡‡æ ·å™¨ï¼Œä¾‹å¦‚vLLM                         |
| å¥–åŠ±                    | twinkle.reward.Reward                      | ç”¨äºå®ç°ä¸åŒRLè®­ç»ƒçš„å¥–åŠ±                      |
| ä¼˜åŠ¿                    | twinkle.advantage.Advantage                | ç”¨äºå®ç°ä¸åŒRLè®­ç»ƒçš„ä¼˜åŠ¿ä¼°è®¡                    |
| æ¨¡æ¿                    | twinkle.template.Template                  | ç”¨äºå¤„ç†æ ‡å‡†è¾“å…¥ï¼Œå¹¶è½¬æ¢æˆæ¨¡å‹éœ€è¦çš„token            |
| æƒé‡åŒæ­¥                  | twinkle.checkpoint_engine.CheckpointEngine | ç”¨äºRLè®­ç»ƒä¸­çš„æƒé‡åŒæ­¥                       |

> æœªåœ¨ä¸Šè¡¨ä¸­åˆ—å‡ºçš„ç»„ä»¶ï¼Œå¦‚Datasetã€DataLoaderç­‰ä¹Ÿå¯ä»¥å®ç°å®šåˆ¶ï¼Œåªéœ€è¦è·ŸéšåŸºç±»APIè®¾è®¡å³å¯ã€‚

## DeviceGroupå’ŒDeviceMesh

DeviceGroupå’ŒDeviceMeshæ˜¯twinkleæ¶æ„çš„æ ¸å¿ƒã€‚æ‰€æœ‰çš„ä»£ç æ„å»ºå‡åŸºäºè¿™ä¸¤ä¸ªè®¾è®¡ã€‚

```python
import twinkle
from twinkle import DeviceMesh, DeviceGroup
device_group = [
        DeviceGroup(
            name='default',
            ranks=8,
            device_type='cuda',
        )
    ]
    
device_mesh = DeviceMesh.from_sizes(pp_size=2, tp_size=2, dp_size=2)
twinkle.initialize(mode='ray', nproc_per_node=8, groups=device_group)
```

å½“device_groupå®šä¹‰å®Œæˆåï¼Œéœ€è¦ä½¿ç”¨`twinkle.initialize`æ¥åˆå§‹åŒ–èµ„æºã€‚

DeviceGroupï¼šå®šä¹‰æœ¬æ¬¡è®­ç»ƒéœ€è¦å¤šå°‘ä¸ªèµ„æºç»„ã€‚å®šä¹‰åï¼Œç»„ä»¶å¯ä»¥é€šè¿‡é€‰æ‹©èµ„æºç»„çš„æ–¹å¼å°†è‡ªå·±è¿è¡Œåœ¨è¿œç«¯ï¼š

```python
from twinkle.model import TransformersModel
model = TransformersModel(model_id='ms://Qwen/Qwen2.5-7B-Instruct', remote_group='default', device_mesh=device_mesh)
# æˆ–è€…
from twinkle.model import MegatronModel
model = MegatronModel(model_id='ms://Qwen/Qwen2.5-7B-Instruct', remote_group='default', device_mesh=device_mesh)
```

DeviceMeshç»™å‡ºäº†æ¨¡å‹ç­‰ç»„ä»¶åœ¨èµ„æºç»„ä¸­çš„æ„å‹ã€‚å¯ä»¥ç†è§£ä¸ºå¦‚ä½•è¿›è¡Œå¹¶è¡Œã€‚è¿™ä¼šå½±å“ä¸€ç³»åˆ—çš„æ¡†æ¶å†³ç­–ï¼Œä¾‹å¦‚å–æ•°æ®ã€æ¶ˆè´¹æ•°æ®ã€æ•°æ®è¿”å›ç­‰ã€‚

## ä½¿ç”¨æ ·ä¾‹

```python
from peft import LoraConfig
import twinkle
from twinkle import DeviceMesh, DeviceGroup
from twinkle.dataloader import DataLoader
from twinkle.dataset import Dataset, DatasetMeta
from twinkle.model import TransformersModel
from twinkle.preprocessor import SelfCognitionProcessor

device_group = [DeviceGroup(name='default',ranks=8,device_type='cuda')]
device_mesh = DeviceMesh.from_sizes(fsdp_size=4, dp_size=2)
# local for torchrun
twinkle.initialize(mode='ray', groups=device_group, global_device_mesh=device_mesh)


def train():
    # 1000 samples
    dataset = Dataset(dataset_meta=DatasetMeta('ms://swift/self-cognition', data_slice=range(1000)))
    # Set template to prepare encoding
    dataset.set_template('Template', model_id='ms://Qwen/Qwen2.5-7B-Instruct')
    # Preprocess the dataset to standard format
    dataset.map(SelfCognitionProcessor('twinkleå¤§æ¨¡å‹', 'ModelScopeç¤¾åŒº'))
    # Encode dataset
    dataset.encode()
    # Global batch size = 8, for GPUs, so 1 sample per GPU
    dataloader = DataLoader(dataset=dataset, batch_size=8, min_batch_size=8)
    # Use a TransformersModel
    model = TransformersModel(model_id='ms://Qwen/Qwen2.5-7B-Instruct', remote_group='default')

    lora_config = LoraConfig(
        r=8,
        lora_alpha=32,
        target_modules='all-linear'
    )

    # Add a lora to model, with name `default`
    # Comment this to use full-parameter training
    model.add_adapter_to_model('default', lora_config, gradient_accumulation_steps=2)
    # Add Optimizer for lora `default`
    model.set_optimizer(optimizer_cls='AdamW', lr=1e-4)
    # Add LRScheduler for lora `default`
    model.set_lr_scheduler(scheduler_cls='CosineWarmupScheduler', num_warmup_steps=5,
                           num_training_steps=len(dataloader))
    for step, batch in enumerate(dataloader):
        # Do forward and backward
        model.forward_backward(inputs=batch)
        # Step
        model.clip_grad_and_step()
        if step % 20 == 0:
            # Print metric
            metric = model.calculate_metric(is_training=True)
            print(f'Current is step {step} of {len(dataloader)}, metric: {metric}')
    model.save(f'last-checkpoint')


if __name__ == '__main__':
    train()
```

è¿™æ ·å¯åŠ¨è®­ç»ƒ:

```shell
python3 train.py
```

