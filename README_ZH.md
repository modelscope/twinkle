<h1 align="center">Twinkle: <b>T</b>raining <b>W</b>orkbench for <b>I</b>ndustrial <b>N</b>eural-network <b>K</b>it & <b>L</b>LM <b>E</b>ngineering</h1>

<p align="center">
    <br>
    <img src="assets/slogan.png"/>
    <br>
<p>
<p align="center">
<a href="https://modelscope.cn/home">The Modelscope Community</a>
<br>
        ä¸­æ–‡&nbsp ï½œ &nbsp<a href="README.md">English</a>&nbsp
</p>

<p align="center">
<img src="https://img.shields.io/badge/python-3.11-5be.svg">
<img src="https://img.shields.io/badge/pytorch-%E2%89%A52.0-orange.svg">
<a href="https://pypi.org/project/twinkle/"><img src="https://badge.fury.io/py/twinkle.svg"></a>
<a href="https://github.com/modelscope/twinkle/blob/main/LICENSE"><img src="https://img.shields.io/github/license/modelscope/twinkle"></a>
<a href="https://pepy.tech/project/twinkle-kit"><img src="https://pepy.tech/badge/twinkle-kit"></a>
<a href="https://github.com/modelscope/twinkle/pulls"><img src="https://img.shields.io/badge/PR-welcome-55EB99.svg"></a>
</p>

<p align="center">
        <a href="https://swift.readthedocs.io/en/latest/">English Documentation</a> &nbsp ï½œ &nbsp <a href="https://swift.readthedocs.io/zh-cn/latest/">ä¸­æ–‡æ–‡æ¡£</a> &nbsp
</p>

<div align="center">

## âœ¨ Twinkleæ˜¯ä»€ä¹ˆï¼Ÿ

å¤§æ¨¡å‹è®­ç»ƒç»„ä»¶åº“ã€‚åŸºäº PyTorchï¼Œæ›´ç®€æ´ã€æ›´çµæ´»ã€ç”Ÿäº§å°±ç»ªã€‚

<p align="center">
ğŸ§© <b>æ¾è€¦åˆæ¶æ„</b> Â· æ ‡å‡†åŒ–æ¥å£<br>
ğŸš€ <b>å¤šè¿è¡Œæ¨¡å¼</b> Â· torchrun / Ray / HTTP<br>
ğŸ”Œ <b>å¤šæ¡†æ¶å…¼å®¹</b> Â· Transformers / Megatron<br>
ğŸ‘¥ <b>å¤šç§Ÿæˆ·æ”¯æŒ</b> Â· å•åŸºåº§æ¨¡å‹éƒ¨ç½²
</p>

</div>

## å®‰è£…

ä½¿ç”¨pipå®‰è£…ï¼š

```shell
pip install 'twinkle-kit'
```

## æºä»£ç å®‰è£…

```shell
git clone https://github.com/modelscope/twinkle.git
cd twinkle
pip install -e . --no-build-isolation
```

## ç¤ºä¾‹æ•™ç¨‹

| è®­ç»ƒç±»å‹                          | æ¨¡å‹æ¡†æ¶     | cookbookåœ°å€                             |
| --------------------------------- | ------------ | ---------------------------------------- |
| FSDP finetuning                   | transformers | [è„šæœ¬](cookbook/transformers/fsdp2.py)      |
| FSDP MoE finetuning               | transformers | [è„šæœ¬](cookbook/transformers/fsdp2_moe.py)  |
| pp/tp/cp finetuning               | megatron     | [è„šæœ¬](cookbook/megatron/tp.py)             |
| pp/tp/cp MoE finetuning           | megatron     | [è„šæœ¬](cookbook/megatron/tp_moe.py)         |
| tinker client finetuning          | megatron     | [è„šæœ¬](cookbook/client/tinker/megatron)     |
| tinker client finetuning/sampling | transformers | [è„šæœ¬](cookbook/client/tinker/transformer)  |
| twinkle client finetuning         | megatron     | [è„šæœ¬](cookbook/client/twinkle/megatron)    |
| twinkle client finetuning         | transformer  | [è„šæœ¬](cookbook/client/twinkle/transformer) |

## æ›´æ–°æ—¥å¿—

- ğŸ‰2026-02-10 twinkle-kitç¬¬ä¸€ç‰ˆç¼–å†™å®Œæˆï¼ŒåŒ…å«çº¯æ–‡æœ¬æ¨¡å‹SFT/PT/RLå’Œè¿œç¨‹è®­ç»ƒèƒ½åŠ›ï¼Œå¹¶æ”¯æŒäº†[é­”æ­å®˜æ–¹å…è´¹èµ„æº]()

## æ”¯æŒçš„ç¡¬ä»¶

| ç¡¬ä»¶ç¯å¢ƒ                    | å¤‡æ³¨                              |
| --------------------------- | --------------------------------- |
| GPU A10/A100/H100/RTXç³»åˆ—ç­‰ |                                   |
| GPU T4/V100ç­‰               | ä¸æ”¯æŒbfloat16ã€Flash-Attention   |
| Ascend NPU                  | éƒ¨åˆ†ç®—å­ä¸æ”¯æŒ                    |
| PPU                         | æ”¯æŒ                              |
| CPU                         | æ”¯æŒdatasetã€dataloaderç­‰éƒ¨åˆ†ç»„ä»¶ |

## æ”¯æŒçš„å¤§è¯­è¨€æ¨¡å‹

| Model Type          | Model ID ä¸¾ä¾‹                                                                                                              | Requires             | Support Megatron | HF Model ID                                                                                                |
| ------------------- | -------------------------------------------------------------------------------------------------------------------------- | -------------------- | ---------------- | ---------------------------------------------------------------------------------------------------------- |
| qwen2 å…¨ç³»åˆ—        | [Qwen/Qwen2-0.5B-Instruct](https://modelscope.cn/models/Qwen/Qwen2-0.5B-Instruct)                                             | transformers>=4.37   | âœ”               | [Qwen/Qwen2-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2-0.5B-Instruct)                                   |
|                     | [Qwen/Qwen2-72B-Instruct](https://modelscope.cn/models/Qwen/Qwen2-72B-Instruct)                                               | transformers>=4.37   | âœ”               | [Qwen/Qwen2-72B-Instruct](https://huggingface.co/Qwen/Qwen2-72B-Instruct)                                     |
|                     | [Qwen/Qwen2-1.5B](https://modelscope.cn/models/Qwen/Qwen2-1.5B)                                                               | transformers>=4.37   | âœ”               | [Qwen/Qwen2-1.5B](https://huggingface.co/Qwen/Qwen2-1.5B)                                                     |
|                     | [Qwen/Qwen2-7B](https://modelscope.cn/models/Qwen/Qwen2-7B)                                                                   | transformers>=4.37   | âœ”               | [Qwen/Qwen2-7B](https://huggingface.co/Qwen/Qwen2-7B)                                                         |
|                     | [Qwen/Qwen2-72B](https://modelscope.cn/models/Qwen/Qwen2-72B)                                                                 | transformers>=4.37   | âœ”               | [Qwen/Qwen2-72B](https://huggingface.co/Qwen/Qwen2-72B)                                                       |
|                     | [Qwen/Qwen2.5-0.5B-Instruct](https://modelscope.cn/models/Qwen/Qwen2.5-0.5B-Instruct)                                         | transformers>=4.37   | âœ”               | [Qwen/Qwen2.5-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct)                               |
|                     | [Qwen/Qwen2.5-1.5B-Instruct](https://modelscope.cn/models/Qwen/Qwen2.5-1.5B-Instruct)                                         | transformers>=4.37   | âœ”               | [Qwen/Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct)                               |
|                     | [Qwen/Qwen2.5-72B-Instruct](https://modelscope.cn/models/Qwen/Qwen2.5-72B-Instruct)                                           | transformers>=4.37   | âœ”               | [Qwen/Qwen2.5-72B-Instruct](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct)                                 |
|                     | [Qwen/Qwen2.5-0.5B](https://modelscope.cn/models/Qwen/Qwen2.5-0.5B)                                                           | transformers>=4.37   | âœ”               | [Qwen/Qwen2.5-0.5B](https://huggingface.co/Qwen/Qwen2.5-0.5B)                                                 |
|                     | [Qwen/Qwen2.5-32B](https://modelscope.cn/models/Qwen/Qwen2.5-32B)                                                             | transformers>=4.37   | âœ”               | [Qwen/Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B)                                                   |
| qwen2_moe å…¨ç³»åˆ—    | [Qwen/Qwen1.5-MoE-A2.7B-Chat](https://modelscope.cn/models/Qwen/Qwen1.5-MoE-A2.7B-Chat)                                       | transformers>=4.40   | âœ”               | [Qwen/Qwen1.5-MoE-A2.7B-Chat](https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B-Chat)                             |
|                     | [Qwen/Qwen1.5-MoE-A2.7B](https://modelscope.cn/models/Qwen/Qwen1.5-MoE-A2.7B)                                                 | transformers>=4.40   | âœ”               | [Qwen/Qwen1.5-MoE-A2.7B](https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B)                                       |
| qwen3 å…¨ç³»åˆ—        | [Qwen/Qwen3-0.6B-Base](https://modelscope.cn/models/Qwen/Qwen3-0.6B-Base)                                                     | transformers>=4.51   | âœ”               | [Qwen/Qwen3-0.6B-Base](https://huggingface.co/Qwen/Qwen3-0.6B-Base)                                           |
|                     | [Qwen/Qwen3-14B-Base](https://modelscope.cn/models/Qwen/Qwen3-14B-Base)                                                       | transformers>=4.51   | âœ”               | [Qwen/Qwen3-14B-Base](https://huggingface.co/Qwen/Qwen3-14B-Base)                                             |
|                     | [Qwen/Qwen3-0.6B](https://modelscope.cn/models/Qwen/Qwen3-0.6B)                                                               | transformers>=4.51   | âœ”               | [Qwen/Qwen3-0.6B](https://huggingface.co/Qwen/Qwen3-0.6B)                                                     |
|                     | [Qwen/Qwen3-1.7B](https://modelscope.cn/models/Qwen/Qwen3-1.7B)                                                               | transformers>=4.51   | âœ”               | [Qwen/Qwen3-1.7B](https://huggingface.co/Qwen/Qwen3-1.7B)                                                     |
|                     | [Qwen/Qwen3-32B](https://modelscope.cn/models/Qwen/Qwen2.5-32B)                                                               | transformers>=4.51   | âœ”               | [Qwen/Qwen3-32B](https://huggingface.co/Qwen/Qwen3-32B)                                                       |
| qwen3_moe å…¨ç³»åˆ—    | [Qwen/Qwen3-30B-A3B-Base](https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Base)                                               | transformers>=4.51   | âœ”               | [Qwen/Qwen3-30B-A3B-Base](https://huggingface.co/Qwen/Qwen3-30B-A3B-Base)                                     |
|                     | [Qwen/Qwen3-30B-A3B](https://modelscope.cn/models/Qwen/Qwen3-30B-A3B)                                                         | transformers>=4.51   | âœ”               | [Qwen/Qwen3-30B-A3B](https://huggingface.co/Qwen/Qwen3-30B-A3B)                                               |
|                     | [Qwen/Qwen3-235B-A22B](https://modelscope.cn/models/Qwen/Qwen3-235B-A22B)                                                     | transformers>=4.51   | âœ”               | [Qwen/Qwen3-235B-A22B](https://huggingface.co/Qwen/Qwen3-235B-A22B)                                           |
| chatglm2 å…¨ç³»åˆ—     | [ZhipuAI/chatglm2-6b](https://modelscope.cn/models/ZhipuAI/chatglm2-6b)                                                       | transformers<4.42    | âœ˜               | [zai-org/chatglm2-6b](https://huggingface.co/zai-org/chatglm2-6b)                                             |
|                     | [ZhipuAI/chatglm2-6b-32k](https://modelscope.cn/models/ZhipuAI/chatglm2-6b-32k)                                               | transformers<4.42    | âœ˜               | [zai-org/chatglm2-6b-32k](https://huggingface.co/zai-org/chatglm2-6b-32k)                                     |
| chatglm3 å…¨ç³»åˆ—     | [ZhipuAI/chatglm3-6b](https://modelscope.cn/models/ZhipuAI/chatglm3-6b)                                                       | transformers<4.42    | âœ˜               | [zai-org/chatglm3-6b](https://huggingface.co/zai-org/chatglm3-6b)                                             |
|                     | [ZhipuAI/chatglm3-6b-base](https://modelscope.cn/models/ZhipuAI/chatglm3-6b-base)                                             | transformers<4.42    | âœ˜               | [zai-org/chatglm3-6b-base](https://huggingface.co/zai-org/chatglm3-6b-base)                                   |
|                     | [ZhipuAI/chatglm3-6b-32k](https://modelscope.cn/models/ZhipuAI/chatglm3-6b-32k)                                               | transformers<4.42    | âœ˜               | [zai-org/chatglm3-6b-32k](https://huggingface.co/zai-org/chatglm3-6b-32k)                                     |
|                     | [ZhipuAI/chatglm3-6b-128k](https://modelscope.cn/models/ZhipuAI/chatglm3-6b-128k)                                             | transformers<4.42    | âœ˜               | [zai-org/chatglm3-6b-128k](https://huggingface.co/zai-org/chatglm3-6b-128k)                                   |
| chatglm4 å…¨ç³»åˆ—     | [ZhipuAI/glm-4-9b-chat](https://modelscope.cn/models/ZhipuAI/glm-4-9b-chat)                                                   | transformers>=4.42   | âœ˜               | [zai-org/glm-4-9b-chat](https://huggingface.co/zai-org/glm-4-9b-chat)                                         |
|                     | [ZhipuAI/glm-4-9b](https://modelscope.cn/models/ZhipuAI/glm-4-9b)                                                             | transformers>=4.42   | âœ˜               | [zai-org/glm-4-9b](https://huggingface.co/zai-org/glm-4-9b)                                                   |
|                     | [ZhipuAI/glm-4-9b-chat-1m](https://modelscope.cn/models/ZhipuAI/glm-4-9b-chat-1m)                                             | transformers>=4.42   | âœ˜               | [zai-org/glm-4-9b-chat-1m](https://huggingface.co/zai-org/glm-4-9b-chat-1m)                                   |
|                     | [ZhipuAI/LongWriter-glm4-9b](https://modelscope.cn/models/ZhipuAI/LongWriter-glm4-9b)                                         | transformers>=4.42   | âœ˜               | [zai-org/LongWriter-glm4-9b](https://huggingface.co/zai-org/LongWriter-glm4-9b)                               |
| glm_edge å…¨ç³»åˆ—     | [ZhipuAI/glm-edge-1.5b-chat](https://modelscope.cn/models/ZhipuAI/glm-edge-1.5b-chat)                                         | transformers>=4.46   | âœ˜               | [zai-org/glm-edge-1.5b-chat](https://huggingface.co/zai-org/glm-edge-1.5b-chat)                               |
|                     | [ZhipuAI/glm-edge-4b-chat](https://modelscope.cn/models/ZhipuAI/glm-edge-4b-chat)                                             | transformers>=4.46   | âœ˜               | [zai-org/glm-edge-4b-chat](https://huggingface.co/zai-org/glm-edge-4b-chat)                                   |
| internlm2 å…¨ç³»åˆ—    | [Shanghai_AI_Laboratory/internlm2-1_8b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-1_8b)                   | transformers>=4.38   | âœ˜               | [internlm/internlm2-1_8b](https://huggingface.co/internlm/internlm2-1_8b)                                     |
|                     | [Shanghai_AI_Laboratory/internlm2-chat-1_8b-sft](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-1_8b-sft) | transformers>=4.38   | âœ˜               | [internlm/internlm2-chat-1_8b-sft](https://huggingface.co/internlm/internlm2-chat-1_8b-sft)                   |
|                     | [Shanghai_AI_Laboratory/internlm2-base-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-base-7b)             | transformers>=4.38   | âœ˜               | [internlm/internlm2-base-7b](https://huggingface.co/internlm/internlm2-base-7b)                               |
|                     | [Shanghai_AI_Laboratory/internlm2-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-7b)                       | transformers>=4.38   | âœ˜               | [internlm/internlm2-7b](https://huggingface.co/internlm/internlm2-7b)                                         |
|                     | [Shanghai_AI_Laboratory/internlm2-chat-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-chat-7b)             | transformers>=4.38   | âœ˜               | [internlm/internlm2-chat-7b](https://huggingface.co/internlm/internlm2-chat-7b)                               |
| deepseek_v1         | [deepseek-ai/deepseek-vl-7b-chat](https://modelscope.cn/models/deepseek-ai/deepseek-vl-7b-chat)                               | transformers>=4.39.4 | âœ”               |                                                                                                            |
|                     | [deepseek-ai/DeepSeek-V2-Lite](https://modelscope.cn/models/deepseek-ai/DeepSeek-V2-Lite)                                     | transformers>=4.39.3 | âœ”               | [deepseek-ai/DeepSeek-V2-Lite](https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite)                           |
|                     | [deepseek-ai/DeepSeek-V2-Lite-Chat](https://modelscope.cn/models/deepseek-ai/DeepSeek-V2-Lite-Chat)                           | transformers>=4.39.3 | âœ”               | [deepseek-ai/DeepSeek-V2-Lite-Chat](https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite-Chat)                 |
|                     | [deepseek-ai/DeepSeek-V2](https://modelscope.cn/models/deepseek-ai/DeepSeek-V2)                                               | transformers>=4.39.3 | âœ”               | [deepseek-ai/DeepSeek-V2](https://huggingface.co/deepseek-ai/DeepSeek-V2)                                     |
|                     | [deepseek-ai/DeepSeek-V2-Chat](https://modelscope.cn/models/deepseek-ai/DeepSeek-V2-Chat)                                     | transformers>=4.39.3 | âœ”               | [deepseek-ai/DeepSeek-V2-Chat](https://huggingface.co/deepseek-ai/DeepSeek-V2-Chat)                           |
|                     | [deepseek-ai/DeepSeek-V2.5](https://modelscope.cn/models/deepseek-ai/DeepSeek-V2.5)                                           | transformers>=4.39.3 | âœ”               | [deepseek-ai/DeepSeek-V2.5](https://huggingface.co/deepseek-ai/DeepSeek-V2.5)                                 |
|                     | [deepseek-ai/DeepSeek-Prover-V2-7B](https://modelscope.cn/models/deepseek-ai/DeepSeek-Prover-V2-7B)                           | transformers>=4.39.3 | âœ”               | [deepseek-ai/DeepSeek-Prover-V2-7B](https://huggingface.co/deepseek-ai/DeepSeek-Prover-V2-7B)                 |
|                     | [deepseek-ai/DeepSeek-R1](https://modelscope.cn/models/deepseek-ai/DeepSeek-R1)                                               | transformers>=4.39.3 | âœ”               | [deepseek-ai/DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)                                     |
| deepSeek-r1-distill | [deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B](https://modelscope.cn/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)           | transformers>=4.37   | âœ”               | [deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B) |
|                     | [deepseek-ai/DeepSeek-R1-Distill-Qwen-7B](https://modelscope.cn/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)               | transformers>=4.37   | âœ”               | [deepseek-ai/DeepSeek-R1-Distill-Qwen-7B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)     |
|                     | [deepseek-ai/DeepSeek-R1-Distill-Qwen-14B](https://modelscope.cn/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)             | transformers>=4.37   | âœ”               | [deepseek-ai/DeepSeek-R1-Distill-Qwen-14B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |
|                     | [deepseek-ai/DeepSeek-R1-Distill-Qwen-32B](https://modelscope.cn/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)             | transformers>=4.37   | âœ”               | [deepseek-ai/DeepSeek-R1-Distill-Qwen-32B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |

## ç¤ºä¾‹ä»£ç 

```python
from peft import LoraConfig
import twinkle
from twinkle import DeviceMesh, DeviceGroup
from twinkle.dataloader import DataLoader
from twinkle.dataset import Dataset, DatasetMeta
from twinkle.model import TransformersModel
from twinkle.preprocessor import SelfCognitionProcessor

device_group = [DeviceGroup(name='default',ranks=8,device_type='cuda')]
device_mesh = DeviceMesh.from_sizes(fsdp_size=4, dp_size=2)
# local for torchrun
twinkle.initialize(mode='ray', groups=device_group, global_device_mesh=device_mesh)


def train():
    # 1000 samples
    dataset = Dataset(dataset_meta=DatasetMeta('ms://swift/self-cognition', data_slice=range(1000)))
    # Set template to prepare encoding
    dataset.set_template('Template', model_id='ms://Qwen/Qwen2.5-7B-Instruct')
    # Preprocess the dataset to standard format
    dataset.map(SelfCognitionProcessor('twinkleå¤§æ¨¡å‹', 'ModelScopeç¤¾åŒº'))
    # Encode dataset
    dataset.encode()
    # Global batch size = 8, for GPUs, so 1 sample per GPU
    dataloader = DataLoader(dataset=dataset, batch_size=8, min_batch_size=8)
    # Use a TransformersModel
    model = TransformersModel(model_id='ms://Qwen/Qwen2.5-7B-Instruct', remote_group='default')

    lora_config = LoraConfig(
        r=8,
        lora_alpha=32,
        target_modules='all-linear'
    )

    # Add a lora to model, with name `default`
    # Comment this to use full-parameter training
    model.add_adapter_to_model('default', lora_config, gradient_accumulation_steps=2)
    # Add Optimizer for lora `default`
    model.set_optimizer(optimizer_cls='AdamW', lr=1e-4)
    # Add LRScheduler for lora `default`
    model.set_lr_scheduler(scheduler_cls='CosineWarmupScheduler', num_warmup_steps=5,
                           num_training_steps=len(dataloader))
    for step, batch in enumerate(dataloader):
        # Do forward and backward
        model.forward_backward(inputs=batch)
        # Step
        model.clip_grad_and_step()
        if step % 20 == 0:
            # Print metric
            metric = model.calculate_metric(is_training=True)
            print(f'Current is step {step} of {len(dataloader)}, metric: {metric}')
    model.save(f'last-checkpoint')


if __name__ == '__main__':
    train()
```

è¿™æ ·å¯åŠ¨è®­ç»ƒ:

```shell
python3 train.py
```

## æ¶æ„è®¾è®¡

<img src="assets/framework.jpg" style="max-width: 500px; width: 100%;" />

twinkleçš„æ¶æ„ç”±clientå’Œserverä¸¤éƒ¨åˆ†æ„æˆï¼Œå…¶ä¸­clientç«¯åŒ…å«ä¸¤ä¸ªä½¿ç”¨å¯èƒ½æ€§ï¼š

1. ç¬¦åˆtwinkleè°ƒç”¨APIçš„å®¢æˆ·ç«¯ï¼Œå…¶APIå’Œserverç«¯å®Œå…¨ç›¸åŒ
2. å¯¹åŸç”ŸTinker APIçš„å…¼å®¹

è¿™ä½¿å¾—å¼€å‘è€…å¯ä»¥ç›´æ¥ä½¿ç”¨Tinker APIè°ƒç”¨twinkleéƒ¨ç½²èµ·æ¥çš„åç«¯è®­ç»ƒæœåŠ¡ã€‚

## æ”¯æŒçš„ç»„ä»¶

|                                                            |                                                          |                                                              |                                                            |                                                                |
| :--------------------------------------------------------: | :-------------------------------------------------------: | :----------------------------------------------------------: | :--------------------------------------------------------: | :-------------------------------------------------------------: |
|  **Dataset**`<br><sub>`æ•°æ®åŠ è½½å’Œé¢„å¤„ç†`</sub>`  |    **Template**`<br><sub>`ç¼–ç å’Œè§£ç `</sub>`    | **DataLoader**`<br><sub>`æ•°æ®åˆ†å‘å’ŒbatchåŒ–`</sub>` |    **Preprocessor**`<br><sub>`æ•°æ®ETL`</sub>`    | **InputProcessor**`<br><sub>`å¤„ç†ä»»åŠ¡ç‰¹å®šè¾“å…¥`</sub>` |
| **Model**`<br><sub>`å¤§æ¨¡å‹ï¼Œæ”¯æŒå¤šç§æ¡†æ¶`</sub>` |      **Sampler**`<br><sub>`é‡‡æ ·å™¨`</sub>`      |          **Loss**`<br><sub>`æ®‹å·®`</sub>`          |    **Metric**`<br><sub>`è®­ç»ƒæŒ‡æ ‡é›†åˆ`</sub>`    |         **Reward**`<br><sub>`å¥–åŠ±å‡½æ•°`</sub>`         |
|     **Advantage**`<br><sub>`ä¼˜åŠ¿å‡½æ•°`</sub>`     | **CheckpointEngine**`<br><sub>`æƒé‡åŒæ­¥`</sub>` |   **Patch**`<br><sub>`è¡¥ä¸ï¼Œç”¨äºæ¨¡å‹ä¿®å¤`</sub>`   | **Module**`<br><sub>`ç»„ä»¶ï¼Œä¾‹å¦‚Optimizer`</sub>` |           **Kernel**`<br><sub>`ç®—å­`</sub>`           |
|    **Server**`<br><sub>`å¼€å¯åç«¯é›†ç¾¤`</sub>`    |     **Client**`<br><sub>`å®¢æˆ·ç«¯ä»£ç `</sub>`     | **Infra**`<br><sub>`éš”ç¦»rayå’Œtorchrunå·®å¼‚`</sub>` |    **Plugin**`<br><sub>`ä½¿ç”¨hubç«¯ç»„ä»¶`</sub>`    |       **Hub**`<br><sub>`å¯¹æ¥HF/MSç½‘ç»œåº“`</sub>`       |

## ç¤¾åŒºç»„ä»¶

| ç»„ä»¶ç±»å‹ | ç»„ä»¶é“¾æ¥                                                                                                 | ç»„ä»¶ä½œç”¨                                                          | ä½œè€…           |
| -------- | -------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------- | -------------- |
| Patch    | [qwen3_moe_transformers4_patch](https://www.modelscope.cn/models/twinkle-kit/qwen3_moe_transformers4_patch) | ä¿®å¤Qwen3 MoEæ¨¡å‹åœ¨FSDP2è®­ç»ƒæ—¶Hangçš„é—®é¢˜ï¼Œå¯¹transformers==4.xç”Ÿæ•ˆ | ModelScopeå®˜æ–¹ |
